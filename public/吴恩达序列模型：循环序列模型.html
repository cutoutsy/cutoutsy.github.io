<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"fengxc.me","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="一些序列的数据：  语音识别 音乐生成 情感分类 DNA序列分析 机器翻译 视频动作识别 命名实体识别  数学符号关于单词的表示，可以构建词汇表，然后通过one-hot进行表示。对于不存在的单词，可以创建一个新的标识来表示。">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达序列模型：循环序列模型">
<meta property="og:url" content="https://fengxc.me/%E5%90%B4%E6%81%A9%E8%BE%BE%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%9A%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B.html">
<meta property="og:site_name" content="会微积分的喵">
<meta property="og:description" content="一些序列的数据：  语音识别 音乐生成 情感分类 DNA序列分析 机器翻译 视频动作识别 命名实体识别  数学符号关于单词的表示，可以构建词汇表，然后通过one-hot进行表示。对于不存在的单词，可以创建一个新的标识来表示。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://7oxhal.com1.z0.glb.clouddn.com/rnn.jpg">
<meta property="og:image" content="http://7oxhal.com1.z0.glb.clouddn.com/rnn_arch.png">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/19cbb2d356a2a6e0f35aa2a946b23a2a.png">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/27afdd27f45ad8ddf78677af2a3eeaf8.png">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/ad9dd74b6ce9bcea14baa289df530d6b.png">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/71a0ed918704f6d35091d8b6d60793e4.png">
<meta property="og:image" content="http://7oxhal.com1.z0.glb.clouddn.com/rnn1vsn.jpg">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/1daa38085604dd04e91ebc5e609d1179.png">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/986226c39270a1e14643e8658fe6c374.png">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/9456d50c55cf0408a3fb2b6e903d85d6.png">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/LSTM.png">
<meta property="og:image" content="http://www.ai-start.com/dl2017/images/48c787912f7f8daee638dd311583d6cf.png">
<meta property="og:image" content="http://7oxhal.com1.z0.glb.clouddn.com/deep_rnns.jpg">
<meta property="article:published_time" content="2018-07-23T20:18:06.000Z">
<meta property="article:modified_time" content="2025-04-10T14:09:26.341Z">
<meta property="article:author" content="会微积分的喵">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="吴恩达">
<meta property="article:tag" content="循环">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://7oxhal.com1.z0.glb.clouddn.com/rnn.jpg">


<link rel="canonical" href="https://fengxc.me/%E5%90%B4%E6%81%A9%E8%BE%BE%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%9A%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B.html">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://fengxc.me/%E5%90%B4%E6%81%A9%E8%BE%BE%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%9A%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B.html","path":"吴恩达序列模型：循环序列模型.html","title":"吴恩达序列模型：循环序列模型"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>吴恩达序列模型：循环序列模型 | 会微积分的喵</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">会微积分的喵</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">我走得很慢，但我从不后退</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7"><span class="nav-number">1.</span> <span class="nav-text">数学符号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">循环神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E4%BD%BF%E7%94%A8%E6%A0%87%E5%87%86%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F"><span class="nav-number">2.1.</span> <span class="nav-text">为什么不使用标准的神经网络？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.2.</span> <span class="nav-text">循环神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.3.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E6%97%B6%E9%97%B4%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.4.</span> <span class="nav-text">通过时间的反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.5.</span> <span class="nav-text">不同类型的循环神经网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90"><span class="nav-number">3.</span> <span class="nav-text">语言模型和序列生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%96%B0%E5%BA%8F%E5%88%97%E9%87%87%E6%A0%B7"><span class="nav-number">4.</span> <span class="nav-text">对新序列采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">5.</span> <span class="nav-text">循环神经网络的梯度消失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83-GRU"><span class="nav-number">6.</span> <span class="nav-text">门控循环单元(GRU)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95GRU"><span class="nav-number">6.1.</span> <span class="nav-text">简单GRU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4GRU"><span class="nav-number">6.2.</span> <span class="nav-text">完整GRU</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E5%8D%95%E5%85%83-LSTM"><span class="nav-number">7.</span> <span class="nav-text">长短期记忆单元(LSTM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8C%E5%90%91RNN-Bidirectional-RNN"><span class="nav-number">8.</span> <span class="nav-text">双向RNN(Bidirectional RNN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Deep-RNNs"><span class="nav-number">9.</span> <span class="nav-text">深层循环神经网络(Deep RNNs)</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">会微积分的喵</p>
  <div class="site-description" itemprop="description">Happy Code, Happy Life</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">204</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fengxc.me/%E5%90%B4%E6%81%A9%E8%BE%BE%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%9A%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="会微积分的喵">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="会微积分的喵">
      <meta itemprop="description" content="Happy Code, Happy Life">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="吴恩达序列模型：循环序列模型 | 会微积分的喵">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达序列模型：循环序列模型
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2018-07-23 20:18:06" itemprop="dateCreated datePublished" datetime="2018-07-23T20:18:06+00:00">2018-07-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-04-10 14:09:26" itemprop="dateModified" datetime="2025-04-10T14:09:26+00:00">2025-04-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>0</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>一些序列的数据：</p>
<ul>
<li>语音识别</li>
<li>音乐生成</li>
<li>情感分类</li>
<li>DNA序列分析</li>
<li>机器翻译</li>
<li>视频动作识别</li>
<li>命名实体识别</li>
</ul>
<h3 id="数学符号"><a href="#数学符号" class="headerlink" title="数学符号"></a>数学符号</h3><p>关于单词的表示，可以构建词汇表，然后通过one-hot进行表示。对于不存在的单词，可以创建一个新的标识<UNK>来表示。</p>
<span id="more"></span>
<h3 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h3><h4 id="为什么不使用标准的神经网络？"><a href="#为什么不使用标准的神经网络？" class="headerlink" title="为什么不使用标准的神经网络？"></a>为什么不使用标准的神经网络？</h4><ul>
<li>输入输出在不同的样例中长度不同</li>
<li>不共享从文本不同位置学到的特征<br>比如在位置1学到Bob是个人名，我们希望在其他地方出现Bob的时候也能识别是个人名</li>
</ul>
<h4 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h4><p>循环神经网络引入了隐状态h(hidden state)的概念，h可以对序列形数据提取特征，接着再转换为输出。</p>
<p><img src="http://7oxhal.com1.z0.glb.clouddn.com/rnn.jpg" alt="隐藏状态"></p>
<p>网络结构如下图：<br><img src="http://7oxhal.com1.z0.glb.clouddn.com/rnn_arch.png" alt="循环网络结构"><br>循环神经网络的限制是在某时刻仅仅只使用了前面时刻的信息，并没有使用后面的信息。</p>
<blockquote>
<p>He said, “Teddy Roosevelt was a great President.”<br>He said, “Teddy bears are on sale!”</p>
</blockquote>
<p>对于上面这两个句子，仅仅使用Teddy前面的信息是无法判断Teddy是否是人名。<br>可以使用双向循环神经网络(BRNN)来解决这个问题。</p>
<h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p><img src="http://www.ai-start.com/dl2017/images/19cbb2d356a2a6e0f35aa2a946b23a2a.png" alt="前向传播"></p>
<p>进行符号简化：</p>
<p><img src="http://www.ai-start.com/dl2017/images/27afdd27f45ad8ddf78677af2a3eeaf8.png" alt="符号化简"></p>
<p>不用使用两个参数矩阵，可以融合成一个参数矩阵。</p>
<h4 id="通过时间的反向传播"><a href="#通过时间的反向传播" class="headerlink" title="通过时间的反向传播"></a>通过时间的反向传播</h4><p>计算损失函数：</p>
<p><img src="http://www.ai-start.com/dl2017/images/ad9dd74b6ce9bcea14baa289df530d6b.png" alt="总体损失"></p>
<p>反向传播计算，更新参数</p>
<p><img src="http://www.ai-start.com/dl2017/images/71a0ed918704f6d35091d8b6d60793e4.png" alt="反向传播"></p>
<p>反向计算时，似乎在进行时空穿梭，从现在回到过去，进行梯度计算和参数更新。</p>
<h4 id="不同类型的循环神经网络"><a href="#不同类型的循环神经网络" class="headerlink" title="不同类型的循环神经网络"></a>不同类型的循环神经网络</h4><p>根据输入输出序列长度的不同，可以把循环神经网络分成不同的类型。<br><strong>多对多结构</strong></p>
<ul>
<li>输入序列长度等于输出序列长度<br>命名实体识别</li>
<li>输入序列长度和输出序列长度不等（seq2seq模型，Encoder-Decoder模型）<br>机器翻译，事实上这一结构就是在机器领域最先提出的。</li>
</ul>
<p><strong>多对一结构</strong><br>输出序列长度为1<br>情感分类</p>
<p><strong>一对一结构</strong><br>输入序列长度和输出序列长度都为1，这就是一个标准的小型神经网络</p>
<p><strong>一对多结构</strong><br>音乐生成<br><img src="http://7oxhal.com1.z0.glb.clouddn.com/rnn1vsn.jpg" alt="一对多结构"></p>
<p><strong>总结</strong><br><img src="http://www.ai-start.com/dl2017/images/1daa38085604dd04e91ebc5e609d1179.png" alt="循环神经网络结构总结"></p>
<h3 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h3><p><strong>语言模型</strong><br>语言模型说得通俗一点就是判断一句话是不是人话。是自然语言中最基础也是最重要的工作之一。</p>
<p><strong>RNN训练语言模型</strong><br>训练集：大的英文句子语料库<br><img src="http://www.ai-start.com/dl2017/images/986226c39270a1e14643e8658fe6c374.png" alt="RNN训练语言模型"></p>
<h3 id="对新序列采样"><a href="#对新序列采样" class="headerlink" title="对新序列采样"></a>对新序列采样</h3><p>在训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。<br>一个序列模型模拟了任意特定单词序列的概率，我们要做的就是对这些概率分布进行采样来生成一个新的单词序列。<br>上面我们都是基于词训练的语言模型，我们也可以在字符级别训练语言模型。<br>使用字符级别的语言模型生成序列优点：不用担心出现未知字符。但是会得到很长的句子，包含很多字符，且训练起来计算成本比较高昂。</p>
<p>生成模型。</p>
<h3 id="循环神经网络的梯度消失"><a href="#循环神经网络的梯度消失" class="headerlink" title="循环神经网络的梯度消失"></a>循环神经网络的梯度消失</h3><p>基本的RNN存在一个很大的问题：梯度消失<br>对于下面这两个句子，单复数主语和动词前后应该保持一致，即句子中有长期的依赖。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The cat, which already ate ......, was full.</span><br><span class="line">The cats, which already ate ......, were full.</span><br></pre></td></tr></table></figure><br>基本RNN不擅长捕获长期依赖效应。<br>对于一个很深的网络来说，从输出$\hat{y}$得到的梯度很难传播回去，很难影响靠前层的权重。<br>RNN同样有这个问题。<br>所以基本的RNN模型会有很多局部影响。输出的值主要受附近值的影响。<br><strong>梯度爆炸</strong><br>随着神经网络层数的增多，梯度不仅可能指数型的下降，也可能指数型的上升。<br>梯度消失在训练RNN时是首要的问题，尽管梯度爆炸也是会出现。<br>梯度爆炸很容易发现，参数会大到崩溃，出现NaN。<br>对于梯度爆炸问题，一个解决方法就是用梯度修剪：观察你的梯度向量，如果大于某个阈值，缩放梯度向量，保证它不会太大。<br>梯度消失的问题更难解决。</p>
<h3 id="门控循环单元-GRU"><a href="#门控循环单元-GRU" class="headerlink" title="门控循环单元(GRU)"></a>门控循环单元(GRU)</h3><h4 id="简单GRU"><a href="#简单GRU" class="headerlink" title="简单GRU"></a>简单GRU</h4><p>改变了RNN的隐藏层，使其更好的捕捉深层连接，并改善了梯度消失问题。<br>GRU单元增加了新的变量c，称为记忆细胞(memory cell)。<br>在时间$t$处，有记忆细胞$c^{&lt; t &gt;}$。对于GRU，$c^{&lt; t &gt;} = a^{&lt; t &gt;}$，但使用不同的标记，但我们说到LSTMs时，这两个会是不同的值。</p>
<p>在每个时间步，我们使用一个候选值重写记忆细胞，即$\tilde{c}^{&lt; t &gt;}$，使用tanh激活函数来计算，</p>
<script type="math/tex; mode=display">\tilde{c}^{<t>} = tanh(W_{c}[c^{<t-1>}, x^t] + b_{c})</script><p>GRU真正核心的思想是有一个门，用$\Gamma _{u}$表示，$u$表示更新门，这是一个0到1之间的值，是使用sigmoid函数得到的。</p>
<script type="math/tex; mode=display">\Gamma _{u} = \sigma (W_{u}[c^{<t-1>}, x^t] + b_{u})</script><p><strong>sigmoid</strong>函数总是非常接近0或者非常接近1的，所以$\Gamma _{u}$在大多数情况下非常接近0或1。<br>使用门决定是否要真的更新$c^{&lt; t &gt;}$。</p>
<script type="math/tex; mode=display">c^{t} = \Gamma _{u}\tilde{c}^{t} + (1 - \Gamma _{u})c^{<t-1>}</script><p>因为$\Gamma _{u}$很容易接近0，所以$c^{&lt; t &gt;}$几乎等于$c^{&lt; t-1 &gt;}$，即使经过很多步，$c^{&lt; t &gt;}$也很好的被维持，这是缓解梯度消失问题的关键。</p>
<h4 id="完整GRU"><a href="#完整GRU" class="headerlink" title="完整GRU"></a>完整GRU</h4><p>对于完整的GRU单元，需要做的改变是增加一个门$\Gamma _{r}$，r可以认为代表相关性(relevance)。下一个$c^{&lt; t &gt;}$的候选值$\tilde c^{&lt; t &gt;}$跟$c^{&lt; t-1 &gt;}$的相关性。</p>
<script type="math/tex; mode=display">\tilde{c}^{<t>} = tanh(W_{c}[\Gamma _{r}c^{<t-1>}, x^t] + b_{c})</script><script type="math/tex; mode=display">\Gamma _{u} = \sigma (W_{u}[c^{<t-1>}, x^t] + b_{u})</script><script type="math/tex; mode=display">\Gamma _{r} = \sigma (W_{r}[c^{<t-1>}, x^t] + b_{r})</script><script type="math/tex; mode=display">c^t = \Gamma _{u}\tilde{c}^{<t>} + (1 - \Gamma _{u})c^{<t-1>}</script><p>关于为什么要使用现在这个完整版本，而不使用上面那个简单版本，是根据多年来研究者们的实际试验来选择的。完整版本的GRU在很多不同的问题上也是非常健壮和实用的。</p>
<p>你可以尝试发明新版本的单元，只要你愿意，但是GRU是一个标准版本，也是最常使用的。</p>
<p>GRU和LSTM是在神经网络结构中最常用的两个具体实例。</p>
<p>GRU在减少一个门电路的前提下，仍然保持和LSTM近似的性能。</p>
<h3 id="长短期记忆单元-LSTM"><a href="#长短期记忆单元-LSTM" class="headerlink" title="长短期记忆单元(LSTM)"></a>长短期记忆单元(LSTM)</h3><p>GRU和LSTM对比：</p>
<p><img src="http://www.ai-start.com/dl2017/images/9456d50c55cf0408a3fb2b6e903d85d6.png" alt="GRU_and_LSTM"></p>
<p>可以看到，LSTM没有相关门$\Gamma <em>{r}$，而是增加了遗忘门$\Gamma </em>{f}$和输出门$\Gamma _{o}$，也没有$a^{&lt; t &gt;} = c^{&lt; t &gt;}$。</p>
<p><img src="http://www.ai-start.com/dl2017/images/LSTM.png" alt="LSTM"></p>
<p>这个LSTM和一般使用的版本会有些不同，最常用的版本可能是门值不仅取决于$a^{&lt; t-1 &gt;}$和$x^{&lt; t &gt;}$，还取决于上一个记忆细胞的值$c^{&lt; t-1 &gt;}$。这称为“偷窥孔连接(peephole connection)”。</p>
<p>对于什么时候用GRU？什么时候使用LSTM？没有统一的准则，LSTM更早出现，而GRU是最近才发明出来的，它可能源于Pavia在更加复杂的LSTM模型中做出的简化。<br>通常是在任务上尝试这两种模型，看哪种模型的效果更好。</p>
<p>GRU的优点是简单，更容易创建一个更大的网络，而且只有两个门，计算上运行得更快。</p>
<p>LSTM更加强大和灵活，因为它有三个门而不是两个。</p>
<p>LSTM在历史进程上是个更优的选择，可以作为默认的尝试。</p>
<h3 id="双向RNN-Bidirectional-RNN"><a href="#双向RNN-Bidirectional-RNN" class="headerlink" title="双向RNN(Bidirectional RNN)"></a>双向RNN(Bidirectional RNN)</h3><p>动机：在上面提到过，命名实体识别中，对某个词的判断关依靠前面的词是不够的，有时还需要依靠后面的词。<br>对于标准的RNN块、GRU块和LSTM块，只要这些构件都是前向的，则不能解决这个问题。</p>
<p><img src="http://www.ai-start.com/dl2017/images/48c787912f7f8daee638dd311583d6cf.png" alt="BRNN"></p>
<p>给定一个输入序列$x^{<1>}$到$x^{<4>}$，首先计算前向的$\vec{a}^{<1>}$，然后计算前向的$\vec{a}^{<2>}$，接着$\vec{a}^{<3>}$，$\vec{a}^{<4>}$。<br>反向序列从计算${\overleftarrow{a}}^{<4>}$开始，反向计算，计算反向的${\overleftarrow{a}}^{<3>}$，最后计算${\overleftarrow{a}}^{<1>}$。<br>把所有这些激活值计算完了就可以预测结果了。</p>
<p>举个例子，预测结果$\hat y^{&lt; t &gt;}$</p>
<script type="math/tex; mode=display">\hat y^{< t >} =g(W_{g}\left\lbrack {\overrightarrow{a}}^{< t >},{\overleftarrow{a}}^{< t >} \right\rbrack +b_{y})</script><p>对于其中的单元，不仅仅是标准RNN单元，也可以是GRU单元或者LSTM单元。</p>
<p>事实上，很多NLP问题，对于大量有自然语言处理问题的文本，有LSTM单元的双向RNN模型是用得最多的。</p>
<p>双向RNN模型的缺点是需要完整的数据的序列，你才能预测任意位置。</p>
<h3 id="深层循环神经网络-Deep-RNNs"><a href="#深层循环神经网络-Deep-RNNs" class="headerlink" title="深层循环神经网络(Deep RNNs)"></a>深层循环神经网络(Deep RNNs)</h3><p>为了学习非常复杂的函数，通常我们会把RNN的多个层堆叠在一起构建更深的模型。</p>
<p><img src="http://7oxhal.com1.z0.glb.clouddn.com/deep_rnns.jpg" alt="Deep RNN"></p>
<p>上面的图就是有3个隐层的新的RNN网络。</p>
<p>比如激活值$a^{[2]<3>}$有两个输入，一个从下面过来的输入，一个从左边过来的输入。</p>
<script type="math/tex; mode=display">a^{[2]<3>} = g(W_{2}^{[2]}[a^{[2]<2>}, a^{[1]<3>}] + b_{a}^{[2]})</script><p>参数$W<em>{2}^{[2]}$和$W</em>{2}^{[2]}$在这一层的计算里都一样，相应的每一层都有自己的参数。<br>这些单元可以是标准的RNN单元，也可以是GRU单元或者LSTM单元。<br>标准的神经网络可能会很深，甚至于100层深，而对于RNN来说，有3层就不少了。很少见到堆叠到100层的网络。<br>但是一种常见的是在每个上面堆叠循环层，但这些层并不水平连接，只是一个深层网络用来预测$y$。</p>
<p>也可以构建深层的双向RNN网络。</p>
<p>从基本的RNN网络，基本的循环单元到GRU，LSTM，再到双向RNN，还有深层版的模型。通过这些可以构建不错的学习序列的模型。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/" rel="tag"># 吴恩达</a>
              <a href="/tags/%E5%BE%AA%E7%8E%AF/" rel="tag"># 循环</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/leetcode-807-Max-Increase-to-Keep-City-Skyline.html" rel="prev" title="Leetcode 807: Max Increase to Keep City Skyline">
                  <i class="fa fa-angle-left"></i> Leetcode 807: Max Increase to Keep City Skyline
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/Leetcode-470-Implement-Rand10-Using-Rand7.html" rel="next" title="Leetcode 470:  Implement Rand10() Using Rand7()">
                  Leetcode 470:  Implement Rand10() Using Rand7() <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">会微积分的喵</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">341k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:10</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
