---
layout: post
title: 聊聊Word2Vec
categories: 算法
date: 2018-08-20 22:47:00
tags:
    - 深度学习
    - nlp
    - word2vec
---
### 写在前面
自然语言处理中，语言是高度抽象的，如何用数学化自然语言是一直以来研究的一个问题。之前都是使用词汇表，one-hot向量来表示词。这种表示方法的一大缺点就是把每个词孤立起来，是的算法对相关词的泛化能力不强。one-hot向量表示词，词与词之间的内积都是0，很难区分词语之间的差别或者相关性。
<!--more-->
还可以根据一些特征还表示一个词，比如特征维度有：性别，年龄，高贵，是否是食物，大小，花费等。所以每个词就是这些特征值组成的一个向量，称为词嵌入。这种高纬特征的表示能够比one-hot更好的表示不同的单词。Word2Vec就是一个学习词嵌入的方式，最终学习的特征不会像前面那些特征那么好理解，但是却能很好的表示词。

之所以叫嵌入的原因在于学习的词的特征向量对应于高维空间里的一个点，好像是把词嵌在高纬空间上。为了可视化，t-SNE算法把这个空间映射到低维空间，这样便于观察。
词嵌入已经是NLP领域最重要的概念之一，好多深度学习网络都要以词嵌入作为输入。

### 使用词嵌入
词嵌入做迁移学习的步骤：
第一步，先从大量的文本集中学习此前如，一个非常大的文本集，或者可以下载网上预训练好的词嵌入模型。
第二步，可以用这些词嵌入模型迁移到你的新的只有少量标注训练集的任务中。替代原来的one-hot向量。
第三步，可以根据新任务的数据集对词嵌入进行调整，只有第二步中有较大数据集的时候可以这么做，如果数据集较小，通常不进行微调。

迁移学习是任务A中有大量数据，而任务B中数据少时，迁移的过程才有用，所以对于很多NLP任务这些都是对的，而对于一些语言模型和机器翻译则不然，因为这些本来就有大量的数据。

词嵌入是有固定的词汇表的。

术语编码和嵌入是可以互换的。

### 词嵌入的特性
词嵌入的一大特性是可以实现类比推理。
根据词嵌入可以推断出，man如果对应woman，那么king对应queue，即$e_{man} - e_{woman} = e_{king} - e_{queue}$。
词嵌入的一个显著成果是，可学习的类比关系的一般性。除了上面的，还能学习国家->首都的关系，国家->货币的关系。

### 嵌入矩阵
学习词嵌入的时候，我们具体在学习什么，实际上是学习一个嵌入矩阵。

假设我们的词汇表含有10000个单词，可能还有一个未知词标记<UNK>。我们要做的就是学习一个嵌入矩阵，它将是一个300x10000的矩阵$E$，当我们需要哪个单词的词向量时，只需将这个矩阵$E$乘以改词对应的one-hot向量即可。

在实践中，用大量的矩阵和向量相乘来计算，效率是很低下的。在实践中会使用一个专门的函数来单独查找矩阵$E$的某列（某行）。

我们来看看再Keras中是如何使用词嵌入矩阵的，在Keras中有一个嵌入层Embedding。
```python
keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)
```
将正整数（索引值）转换为固定尺寸的稠密向量，该层只能用作模型中的第一层。
**例子**
```python
model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
# 模型将输入一个大小为 (batch, input_length) 的整数矩阵。
# 输入中最大的整数（即词索引）不应该大于 999 （词汇表大小）
# 现在 model.output_shape == (None, 10, 64)，其中 None 是 batch 的维度。

input_array = np.random.randint(1000, size=(32, 10))

model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)
```
**参数**
* input_dim: int > 0。词汇表大小，即最大整数index + 1。
* output_sim: int > 0。词向量的维度。
* embedding_initializer: embeddings 矩阵的初始化方法

### 学习词嵌入
在深度学习应用于学习词嵌入的历史上，人们一开始使用的算法比较复杂，但随着时间推移，研究者们不断发现他们能用更加简单的算法来达到一样好的效果，特别是在数据集很大的情况下。
早期是使用神经网络语言模型来学习词嵌入矩阵，通过一个固定的历史窗口，固定神经网络的输入，然后用反向传播来进行梯度下降来最大化训练集似然，通过数据集给定的4个单词去重复地预测语料库中的下一个单词。

对于语言模型，一般选择预测词之前的几个单词作为上下文，但是如果不是为了学习语言模型，比如说学习词嵌入，那么可以有其他的选取上下文的方式。
* 预测单词的左边4个词和右边4个词
* 预测词之前的最后一个单词
* 附近的一个词（skip-gram模型的思想）

### Huffman编码
Huffman编码主要是用较小的比特位表示频率较高的字符，而用较多的比特位表示频率较低的字符。

#### 等长编码
等长编码即是每个字母的编码长度是一样的。
比如要编码6个字符：A, E, R, T, F, D，那么可以采用3为2进制（$2^3 = 8 > 6$），可分别用001，001，010，011，100，101来对这6个字母进行编码，然后当对方接受报文时再按照3位一分进行译码。
显然编码的长度取决报文中不同字符的个数。若报文中可能出现26个不同字符，则固定编码长度为5($2^5$ = 32 > 26)。然而报文传送时总是希望总长度尽可能段。在实际应用中，各个字符的出现频度和使用次数是不相同的，如A、B、C的使用频率远远高于X、Y、Z，自然会想到设计编码时，让使用频率高的用短码，使用频率低的用长码，以优化整个报文编码。

#### 前缀编码
前缀编码即要求一个字符的编码不能是另一个字符编码的前缀。可用字符集中的每个字符作为叶子节点生成一颗编码二叉树，为了获得传送报文的最短长度，可将每个字符的出现频率作为字符节点的权值赋予该节点上，显然字符使用频率越小权值越小，权值越小叶子就越靠下，于是频率小编码长，频率高编码短，这样就保证了此树的最小带权路径长度，效果上就是传送报文的最短长度。

#### Huffman编码
利用Huffman树设计的二进制前缀编码，称为Huffman编码，它既能满足前缀编码的条件，又能保证报文编码总长最短。


### Word2Vec
Word2Vec算法是一种简单而且计算时更加高效的方式来学习词嵌入。

#### Skip-Gram模型
给定一个句子，要做的是抽取上下文和目标词配对，来构造一个监督学习问题，上下文是最近的n个词。随机选择一个词作为上下文词，然后在上下文词前后5个词或者10个词内，选择目标词。
这个监督学习问题，给定上下文，要求你预测在这个词左右10个词距或者5个词距内随机选择的某个目标词。显然，这不是个非常简单的学习问题，因为在左右10个词距之间，会有很多不同的单词。但是构造这个监督学习问题的目标并不是想要解决这个监督学习问题本身，而是想要使用这个学习问题来学到一个好的词嵌入模型。

<!-- ![Skip-gram Model](http://7oxhal.com1.z0.glb.clouddn.com/blog_word2vec_skip-gram.png) -->
<img src="http://7oxhal.com1.z0.glb.clouddn.com/blog_word2vec_skip-gram.png" width="50%" height="50%"/>

为了表示输入，单词使用one-hot向量表示，写作$O_{c}$，拿嵌入矩阵$E$乘以向量$O_{c}$，得到输入的上下文词的嵌入向量。即$e_{c} = EO_{c}$。然后把向量$e_{c}$喂入一个softmax单元，softmax单元要做的就是输出$\hat y$，预测不同目标词的概率：
$$Softmax: p(t|c) = \frac{e^{\theta_{t}^Te_{c}}}{\sum_{j=1}^Ve^{\theta_{j}^Te_{c}}}$$

如何对上下文c进行采样，如果使用均匀且随机的采样，那么一些像the、of、a、and等会出现得很频繁。所以实际上并不是单纯的在训练集语料上均匀且随机的采样得到的，而是采用了不同的分级来平衡更常见的词和不那么常见的词。

#### 分级(hierarchical) Softmax分类器
存在问题：上面的分母当词汇表很大时，计算是很慢的。
直观的理解就是，分级分类器不会一下子就确定到底是属于10000类中的哪一类，比如会告诉你，是在词汇表的前5000个中还是在后5000个中，依次类推，知道找到一个叶子节点。树上内部的每一个节点都可以是一个二分类器比如逻辑回归分类器。用这样的分类树，计算成本与词汇表大小的对数成正比，而不是词汇表大小的线性函数。

在实践中，分级softmax分类器不会使用一颗完美平衡的分类树或者说一颗左边和右边分支的词数相等的对称数。实际上，分级的softmax分类器会被构造层常用词在顶部，然而不常用的词在树的更深处。

这儿的分级Softmax分类器，就是采用Huffman编码来对词汇表中的单词进行编码，根据词出现的频率。
**符号声明**
考虑Huffman树中的某个叶子结点，假设它对应词典V中的词W，记
* $p^w$：从根结点出发到达w对应叶子节点的路径
* $l^w$：路径$p^w$中包含结点的个数
* $p_{1}^w,p_{2}^w,..., p_{l^w}^w$：路径$p^w$中的$l^w$个节点，其中$p_{1}^w$表示根结点，$p_{l^w}^w$表示词$w$对应的结点
* $d_{2}^w, d_{3}^w,...,d_{l^w}^w \in \{0,1\}$：词w的Huffman编码，它由$l^w - 1$位编码构成，$d_{j}^w$表示路径$p^w$中第$j$个结点对应的编码（根结点不对应编码）。
* $\theta_{1}^w, \theta_{2}^w,...,\theta_{l^w-1}^w \in R^m$：路径$p^w$中非叶子结点对应的向量，$\theta_{j}^w$表示路径$p^w$中第$j$个非叶子结点对应的向量。

按理说，我们要求的是词典V中每个词（即Huffman树中所有叶子结点）的向量，为什么这里还要为Huffman树中每一个非叶子结点也定义一个同长的向量呢？事实上，它们只是算法中的辅助向量。

**例子**
假设我们预测的目标词为w = "足球"，

![huffman](http://7oxhal.com1.z0.glb.clouddn.com/blog_word2vec_huffman.png)

图中由4条红色边串起来的5个结点就构成了路径$p^w$，其中长度$l^w = 5$。$p_{1}^w,p_{2}^w,p_{3}^w,p_{4}^w, p_{5}^w$为路径上的5个结点。$d_{2}^w, d_{3}^w,d_{4}^w,d_{5}^w$分别为1，0，0，1，即“足球”的Huffman编码为1001，此外，$\theta_{1}^w,\theta_{2}^w,\theta_{3}^w,\theta_{4}^w$分别表示路径上4个非叶子节点对应的向量。

所以如何利用向量$e_{w}\in R^m$以及Huffman树来定义函数$p(w|context(w))$。其实从根结点出发到达"足球"这个叶子节点，中间共经历了4次分支，而每次分支都可视为进行了一次二分类。我们可以指定标签为1是正类，标签为0是负类，也可以反着来，word2vec源码就是使用的后者。
根据逻辑回归，我们可以在每个节点进行二分类，然后把所有概率乘积起来即得到了$p(w|context(w))$。

**小结一下**
对于词典V中的任意词w, Huffman树中必存在一条从根节点到词w对应结点的路径$p^w$(且这条路径是唯一的)。路径$p^w$上存在$l^w-1$个分支，将每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来，就是所需的$p(w|context(w))$。

#### 负采样（Negative Sampling）
构造一个新的学习问题：给定一对单词，去预测这是否是一对上下文词-目标词。
比如对于句子：I want a glass of orange juice to go along with my cereal.
构造如下数据集：采样得到一个上下文词和一个目标词，生成一个正样本，再在字典中随机选一个词生成负样本，即使这个词出现在上下文词的范围内也没有关系。

| context | word  | target|
|:-----      |:-----    |:-----    |
|orange   | juice | 1     |
|orange   | king  | 0     |
|orange   | book  | 0     |
|orange   | the   | 0     |
|orange   | of    | 0     |

训练集生成1个正样本的同时生成k个负样本，小数据集k取5到20比较好，大数据集可以取小一点，比如2到5。

**模型**
这个模型属于逻辑回归模型，使用符号$c$表示上下文词，符号$t$表示可能的目标词，用$y$表示0和1，表示是否是一对上下文-目标词。
$$P(y=1|c, t) = \sigma (\theta_{t}^Te_{c})$$
参数和之前一样，你对每一个可能的目标词有一个参数向量$\theta_{t}$和另一个参数向量$e_{c}$，即每一个可能上下文词的嵌入向量，我们都将用这个公式估计y=1的概率，通过每个正样本都有k个负样本来训练一个类似逻辑回归的模型。

这样的话算法的复杂度就大大降低了，因为每次只需要更新k+1个logistic单元。

对于负样本的选取，即在选取了上下文词后，如何对这些词进行采样负样本。采用如下方式
$$P(w_{i}) = \frac{f(w_{i}^{\frac{3}{4}})}{\sum_{j=1}^Vf(w_{j})^{\frac{3}{4}}}$$
进行采样，$f(w_{i})$是观测到的在余量库中某个英文词的词频。

#### CBOW模型
CBOW模型和Skip-Gram模型是相反的，CBOW模型是在已知当前词$w_{t}$的上下文$w_{t-1}, w_{t-2}, w_{t+1}, w_{t+2}$的前提下预测当前词$w_{t}$。

<!-- ![CBOW Model](http://7oxhal.com1.z0.glb.clouddn.com/blog_word2vec_cbow.png) -->
<img src="http://7oxhal.com1.z0.glb.clouddn.com/blog_word2vec_cbow.png" width="50%" height="50%" />

对于CBOW模型，输入层包括上下文所有单词的向量求累加和$x_{w}$，然后输入到网络中。输出层可以采用分级softmax来进行优化。那么在反向传播计算的时候，$x_{w}$是表示的各词词向量的累加和，怎么更新单个的词向量呢。这里是对每个单词采用如下的公式进行计算：
$$e(\hat w) = e{\hat w} + \eta \sum_{j=2}^{l^w}\frac{\partial L(w, j)}{\partial x_{w}}$$
即把$\eta \sum_{j=2}^{l^w}\frac{\partial L(w, j)}{\partial x_{w}}$贡献到上下文每一个词的词向量上。

对于使用负采样进行优化时，模型基本和Skip-gram差不多，采样一个上下文词和当前词构成正样本，再在词典中随机选一个词生成负样本，即使这个词出现在上下文词的范围内也没有关系。
利用逻辑回归来进行模型的训练。


### 参考文献
[自然语言处理与词嵌入](http://www.ai-start.com/dl2017/html/lesson5-week2.html#header-n169)
[https://blog.csdn.net/itplus/article/details/37969519](https://blog.csdn.net/itplus/article/details/37969519)