---
layout: post
title: 逻辑回归公式推导
categories: 算法
date: 2018-09-21 10:36:57
tags:
    - 机器学习
    - 逻辑回归
---
### 写在前面
最近忙着秋招，到处去笔试、面试，也没有更新博客，到今天秋招最忙的时间已经过去了，后面可能面试就很少了。也有时间来对之前面试过程中的一些不足进行总结下。

本篇博客主要是关于逻辑回归的公式推导，之前一直没有仔细看这块，知道大概是怎么推导的，今天抽上午看了下，在此做个记录。

有的东西你听着觉得很复杂，其实真正去了解它，并没有你想的那么复杂。
<!--more-->
### 公式推导
首先，简单了解下逻辑分布，设X是连续随机变量，X服从逻辑分布是指X具有下列分布函数和密度函数：
$$F(x) = P(X\leq x) = \frac{1}{1+e^{-(x-\mu )/\gamma}}$$
$$f(x) = F^{'}(x)= \frac{e^{-(x-\mu )/\gamma }}{\gamma (1+e^{-(x-\mu )/\gamma)^2 }} 
$$
式中，$\mu $为位置参数，$\gamma > 0$为形状参数。

其中分布函数属于逻辑斯谛函数，其图形是一条S形曲线。该曲线以点($\mu $, $\frac{1}{2}$)为中心对称，值域是(0, 1)。

逻辑回归就是基于分布函数，二项逻辑回归模型是如下的条件概率分布：
$$P(Y=1|x) = \frac{exp(w\cdot x + b)}{1+exp(w\cdot x + b)}$$
$$P(Y=0|x) = \frac{1}{1+exp(w\cdot x + b)}$$

如果把b也加到x向量中去，那么上面可简化为：
$$P(Y=1|x) = \frac{exp(w\cdot x)}{1+exp(w\cdot x)}$$
$$P(Y=0|x) = \frac{1}{1+exp(w\cdot x)}$$

**参数估计**
逻辑回归的参数估计使用极大似然估计。
假设：
$$P(Y=1|x) = \pi(x)$$
$$P(Y=0|x) = 1 - \pi(x)$$
那么似然函数为:
$$\prod_{i=1}^{N}[\pi(x_{i})]^{y_{i}}[1-\pi(x_{i})]^{1-y_{i}}$$
取对数得到对数似然函数为：
$$\begin{align}
L(w) &= \sum_{i=1}^N [y_{i}log\pi(x_{i})+(1-y_{i})log(1-\pi(x))] \\
&= \sum_{i=1}^N [y_{i}log\frac{\pi(x_{i})}{1-\pi (x_{i})}+log(1-\pi(x_{i}))] \\
&= \sum_{i=1}^N [y_{i}(w\cdot x_{i})-log(1+exp(w\cdot x_{i}))]
\end{align}$$

对$L(w)$求极大值，得到$w$的估计值。

对于目标函数的最优化问题，可以采用梯度下降法和拟牛顿法。

### 逻辑回归和线性回归的区别
首先，逻辑回归处理的是分类问题，而线性回归处理的是回归问题。
一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率是$log\frac{p}{1-p}$。

对逻辑回归的公式进行整理可以得到：
$$log\frac{P(Y=1|x)}{1-P(Y=1|x)} = w\cdot x$$

所以逻辑回归可以看做是对Y=1这一事件的对数几率的线性回归，这也是“逻辑回归”这一称谓的由来。


